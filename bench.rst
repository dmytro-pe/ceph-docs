***********
Бенчмаркинг
***********

Важным моментом при бенчмаркинге является правильный выбор настроек и железа
на КЛИЕНТСКОМ компьютере. Для бенчмаркинга затрагивающего сеть лучше всего
запускать тест на compute-node, или другими словами, на том сервере где будет
работать клиентское по отношению к Ceph приложение.

.. include:: bench-disks.rst

Бенчмаркинг сети
================

* На каждом ноде запускаем iperf3-сервер командой ``iperf3 -s``.
  https://arr4.wordpress.com/2016/04/21/iperf3-on-fedora-23-as-a-service ?

* На сервере, который будет потенциальным клиентом запускаем ``iperf3 -с YOUR_NODE_IP``.
  Для тестировния внутрикластерной сети -- аналогично, но запускаем клиентскую часть
  на одном из нодов. Чтобы протестировать трафик в противоположном направлении, есть
  ключ ``-R``. Для теста UDP нужны дополнительные параметры клиента ``-u -b 0``.

* TODO: как инетпретировать результаты.

Бенчмаркинг Rados
=================

IOPS через rados bench в режиме, соответствующем RBD (4 Кб блоки в 4 Мб объектах)
и 16 одновременно запущенных виртуалок:

.. code-block:: sh

  rados bench -p YOUR_POOL -t 16 -b 4096 -o $((4096 * 1024)) 60 write

Ввиду выполняемых запросов одновременно на нескольких дисках,
тест должен показать "хорошие" результаты. Зависит от многих факторов
(сети, CPU, количества реплик, Erasure Coding, версии Ceph и фазы Луны).

Бенчмаркинг отдельного OSD
==========================

.. code-block:: sh

ceph tell osd.N bench [TOTAL_DATA_BYTES] [BYTES_PER_WRITE]

Тест почти бессмысленный, ибо выполняется отдельным ОСД без учёта сети со
странными паттернами нагрузки. Имеет смысл сравнивать полученные цифры с fio
на этих же дисках.

Бенчмаркинг отдельного OSD через librados
=========================================

IOPS при записи в объект всегда равно IOPS самого медленного OSD среди acting set
соответствующей PG. Поэтому важно найти такие медленные OSD. Для этого написана
специальная утилита: https://github.com/socketpair/ceph-bench

Для работы этой утилиты нужен пул размером 1 с тем же правилом что и production пул.
Утилита работает через librados (не используя RBD), подбирает для тестов такие
названия объектов чтобы они лежали на конкретных OSD и производит запись в них.
Утилита на питоне, так что для all-flash стораджей и 10 Гбит/с может упереться в CPU.

Бенчмаркинг RBD
===============

Производительность может отличаться на заполненном (allocated) и незаполненном
RBD-образе. TODO: Написать как БЫСТРО заполнить тестовый образ. Учесть сжатие в
Bluestore.

* Прямой через rbd bench
  (https://tracker.ceph.com/projects/ceph/wiki/Benchmark_Ceph_Cluster_Performance)

* Прямой (fio + librbd). Говорят, fio врёт с драйвером librbd.

* Замапленный (fio поверх /dev/rbd0)

* Fio изнутри виртуалки. Параллельной нагрузки она не создаст -- qemu rbd
  драйвер работает в один поток. Это непроверенная информация.
