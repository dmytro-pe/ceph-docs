**********
CloudMouse
**********

.. image:: _static/cloudmouse.png
   :alt: Логотип
   :align: left

VPS-хостинг. Год основания -- 2013. Отличились тем, что использовали Ceph крайне неумело,
что привело 10.02.2015 к резкой гибели их бизнеса в связи с полной потерей данных (бекапы
они не делали).
Потом везде на форумах кричали, что Ceph -- глючное поделие и именно он привёл к проблеме.
Эта ситуация нанеслa огромный урон репутации Ceph как надёжного хранилища данных.

Между тем, в Сети крайне трудно найти информацию о том что же именно произошло. Эта ситуация
сейчас используется при каждом удобном и неудобном случае когда кто-то хочет сказать что у
Ceph есть проблемы.

.. pull-quote::

    Недавно, на выставке РИТ++, мне довелось поговорить со специалистом из компании, которая работает на рынке облачного хостинга под брендом flops.ru. Эта компания известна тем, что использует как раз Ceph и построила на нем платформу публичного облачного хостинга. Интересные ребята, кстати, энтузиасты и большие молодцы, серьезно. Я воспользовался случаем узнать «из первых рук» их компетентное мнение о том, насколько реально «скачать Ceph и сделать все то же самое на нем». Ответ был однозначен: «Это утопия».
    Так, в частности, он рассказал, что они в flops потратили на доводку, допиливание и вычищение багов примерно полтора года труда их команды разработчиков. Они взяли довольно старую версию (v 0.67 «Dumpling») и «пилили» ее. И в итоге действительно допилили ее до продакшна. Текущая же версия, в том виде, в котором она выложена в паблик, несмотря на major и stable в названии, в продакшн непригодна.
    «Помните историю cloudmouse, который, в результате слова бага и сбоя в марте этого года потерял данные 22 тысяч виртуальных машин (включая их бэкапы, а перед этим они теряли данные в феврале) и, в результате, ушел из бизнеса? Ребята взяли текущую версию Ceph, и сделали свою систему на ней. Результат — вот такой.»
    Собственно мне больше нечего добавить к этому ответу на вопрос, «можно ли скачать Ceph и сделать бесплатно то же самое, что у вас за деньги?».
    Можно. Но не сразу, не быстро, и, по факту, не бесплатно.

    -- http://blog.in-a-nutshell.ru/nutanix-vs-ceph/

.. pull-quote::

    В результате некого «аппаратного сбоя» компания потеряла около 22 000 виртуальных машин своих
    клиентов без возможности восстановления. При этом ни о какой компенсации речи и не шло, при
    этом, техническая поддержка держала своих клиентов в неведении около 1,5 суток.

    -- https://1spla.ru/blog/kak_ne_nujno_delati_oblako

.. pull-quote::

    Честно вам сказать из-за чего развалился? Ибо сам даже смотрел их кластер и пытался
    помочь... Помнится мне, что тогда еще актуальная версия Ceph'а была Firefly.
    Но им жутко не нравилась его производительность. Ну они и обновились, на в то время,
    не стабильный hammer. А когда пошли ошибки, то потом ещё и обновились аж на master ветку.
    А у мастер ветки была проблема связанная с утечкой памяти, ну и при нехватке памяти на
    сервере (еще один баг) убились данные. И количество репликаций у них было 2. Хотя и
    количество репликации бы тут не помогли. Так что Ceph там был не при чём. Банально админы
    накосячили, ну или их руководство.

    -- Аноним.


.. pull-quote::

    И вроде еще вместо бекапов снапшоты использовали.

    -- `@socketpair <https://t.me/socketpair>`_

.. pull-quote::

    Спасибо всем пользователям, кто понял ситуацию, остался и создают виртуальные машины.
    Они действительно стали работать в 10-40раз быстрее, и это не связано с нагрузкой на облако.

    По поводу СХД, мы используем как минимум 8, с 3х кратной репликацией данных.
    Те "кусочек данных размером 1мб" хранится на 3х разных дисках на 3х СХД. А все данные хранятся
    примерно на 300+х дисках. Как мы ранее сообщали, проблема потери данных была связана с
    аппаратно-програмным сбоем в связки: Ceph+rbd+osd+pg. Другими словами, все связи между блоками
    данных в кластере были утеряны.

    Еще раз, приносим свои извинения.
    По обращению в тикеты, мы помогаем настраивать сервера.
    По поводу компенсаций, мы планируем их зачислить на балансы в ближайшие дни.

    P.s. мы понимаем свою вину перед пользователи, но уверяем вас, мы нашли причины и исправили их.
    А многим облачным компаниям, еще только предстоит ее найти.

    -- https://searchengines.guru/showpost.php?p=13490140

.. pull-quote::

    Судя по тому, что никаких технических подробностей этого падения они не привели,
    косяк на сто процентов их, а не Ceph'a. Очевидно, что был бы это глюк Ceph'a они
    бы молчать не стали, защитили бы свою репутацию.

    -- https://www.linux.org.ru/news/opensource/11500740/page1#comment-11503052

Ccылки:

    * https://habrahabr.ru/post/252221
    * https://habrahabr.ru/post/250097
    * https://1spla.ru/blog/kak_ne_nujno_delati_oblako
    * https://go.backupland.com/crash/clodemouse_deleted_vds/clodemouse_deleted_vds.html
